# GBDT + LR

## 一、GBDT + LR 简介

针对协同过滤和矩阵分解存在的劣势，忽略用户自身特征，物品自身特征，以及上下文信息。

GBDT + LR是2014年由Facebook提出的模型，利用GBDT自动特征筛选和组合，生成新的离散特征向量，并以这个特征向量作为LR模型的输入，来产生最后的预测结果，综合利用用户、物品、上下文多种不同的特征，生成较为全面的推荐结果，在CTR点击率预估场景下使用广泛

## 二、逻辑回归模型

相对于传统方法，逻辑回归模型更能生成更能全面的结果，这里仅介绍比较重要的细节和在推荐中的应用

逻辑回归在线性回归的基础上，加上了一个sigmoid函数（非线性）映射，舍得逻辑回归成为了一个优秀的分类算法，学习逻辑回归模型：**逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，达到数据二分类的目的**

将问题转为分类问题，通过预测正样本概率对物品排序：正样本---可以是用户点击了某个商品或观看了某个视频，均是推荐系统希望用户产生的正反馈行为，逻辑回归模型将推荐问题转化为了一个点击率预估问题（典型二分类问题，正好适合逻辑回归处理）过程如下：

> 1. 将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转化为数值向量
> 2. 确定逻辑回归的优化目标，如把点击率预测转换成二分类问题，这样就可以得到分类问题常用的损失作为目标，训练模型
> 3. 在预测的时候将特征向量输入模型产生预测，得到用户点击物品的概率
> 4. 利用点击概率对候选物品排序，得到推荐列表

这里的关键就是每个特征的权重参数$w$， 我们一般是使用梯度下降的方式， 首先会先随机初始化参数$w$，  然后将特征向量（也就是我们上面数值化出来的特征）输入到模型， 就会通过计算得到模型的预测概率， 然后通过对目标函数求导得到每个$w$的梯度，  然后进行更新$w$

这里的目标函数长下面这样：

$$ J(w)=-\frac{1}{m}\left(\sum_{i=1}^{m}\left(y^{i} \log  f_{w}\left(x^{i}\right)+\left(1-y^{i}\right) \log  \left(1-f_{w}\left(x^{i}\right)\right)\right)\right. $$ 求导之后的方式长这样： $$ w_{j} \leftarrow w_{j}-\gamma \frac{1}{m}  \sum_{i=1}^{m}\left(f_{w}\left(x^{i}\right)-y^{i}\right) x_{j}^{i} $$ 这样通过若干次迭代， 就可以得到最终的$w$了， 关于这些公式的推导，可以参考下面给出的文章链接， 下面我们分析一下逻辑回归模型的优缺点。

推断过程如图：

![](https://www.pinterest.com/0cbe641f-6034-427d-b0b2-c15a8c8e4f4a)

**优点：**

1. LR模型形式简单，可解释性好，从特征的权重可以看到不同的特征对最后结果的影响。
2. 训练时便于并行化，在预测时只需要对特征进行线性加权，所以**性能比较好**，往往适合处理**海量id类特征**，用id类特征有一个很重要的好处，就是**防止信息损失**（相对于范化的 CTR 特征），对于头部资源会有更细致的描述
3. 资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。
4. 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)

**当然， 逻辑回归模型也有一定的局限性**

1. 表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（这些工作都得人工来干， 这样就需要一定的经验， 否则会走一些弯路）， 因此可能造成信息的损失
2. 准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行**离散化**（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。
4. LR 需要进行**人工特征组合**，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。

所以如何**自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期**，是亟需解决的问题， 而GBDT模型， 正好可以**自动发现特征并进行有效组合**

## 三、GBDT模型

GBDT全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征， 所以这个模型依然是一个非常重要的模型。

GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下：

[![img]()https://i.pinimg.com/564x/7f/45/8b/7f458b40ec317981b7afced9c7719bf2.jpg    

gbdt通过多轮迭代， 每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。  gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差。 因为训练的过程是通过降低偏差来不断提高最终分类器的精度。  由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。   

关于GBDT的详细细节，依然是可以参考下面给出的链接。这里想分析一下GBDT如何来进行二分类的，因为我们要明确一点就是**gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的**， 而这里的残差指的就是当前模型的负梯度值， 这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而**gbdt 无论用于分类还是回归一直都是使用的CART 回归树**， 那么既然是回归树， 是如何进行二分类问题的呢？

GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值，  但是二分类问题和回归问题的损失函数是不同的， 关于GBDT在回归问题上的树的生成过程， 损失函数和迭代原理可以参考给出的链接，  回归问题中一般使用的是平方损失， 而二分类问题中， GBDT和逻辑回归一样， 使用的下面这个：
$$
L=\arg \min \left[\sum_{i}^{n}-\left(y_{i} \log  \left(p_{i}\right)+\left(1-y_{i}\right) \log  \left(1-p_{i}\right)\right)\right]
$$
其中， y_i 是第 i 个样本的观测值， 取值要么是 0 要么是 1， 而 p_i 是第 i 个样本的预测值，  取值是 0-1 之间的概率，
$$
由于GBDT拟合的残差是当前模型的负梯度，这个模型的导数，  即\frac{dL}{dp_i}
$$

$$
对于某个特定的样本， 求导的话就可以只考虑它本身， 去掉加和号\frac{dl}{dp_i}
$$

$$
\begin{aligned} l &=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log  \left(1-p_{i}\right) \\ &=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i}  \log \left(1-p_{i}\right) \\ &=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log  \left(1-p_{i}\right) \end{aligned}
$$

$$
\left(\log  \left(\frac{p_{i}}{1-p_{i}}\right)\right) : 对几率比取了个对数
$$

$$
逻辑回归里，这个式子会等于\theta X
$$

$$
推出：p_i=\frac{1}{1+e^-{\theta X}}
$$

$$
\eta_i=\frac{p_i}{1-p_i}, 即p_i=\frac{\eta_i}{1+\eta_i}
$$

$$
\begin{aligned} l &=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{e^{\log  \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}\right) \\ &=-y_{i} \log \left(\eta_{i}\right)-\log \left(\frac{1}{1+e^{\log  \left(\eta_{i}\right)}}\right) \\ &=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log  \left(\eta_{i}\right)}\right) \end{aligned}
$$

$$
对log(\eta_i)求导， 得  \frac{d l}{d \log (\eta_i)}=-y_{i}+\frac{e^{\log  \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_i+p_i
$$

 我们就得到了某个训练样本在当前模型的梯度值了， 那么残差就是y_i - p_i。

GBDT二分类的这个思想，其实和逻辑回归的思想一样，
$$
逻辑回归是用一个线性模型去拟合P(y=1|x)这个事件的对数几率log\frac{p}{1-p}=\theta^Tx
$$

$$
GBDT二分类也是如此， 用一系列的梯度提升树去拟合这个对数几率， 其分类模型可以表达为： P(Y=1 \mid x)=\frac{1}{1+e^{-F_{M}(x)}}
$$

